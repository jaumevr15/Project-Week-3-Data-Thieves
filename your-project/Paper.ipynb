{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Exploration of Porn Statistics\n",
    "Mar√≠a Platas, Alieldin Ramadan, Jaume Vicens\n",
    "November 2019 - Ironhack Barcelona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Porn is one of the most controversial elements in today's Internet. Known by most but openly admitted by a very few. Porn websites are source of huge, enormous databases of content, categories, users and algorithms that can be compared to the ones from most accessible sites like Youtube or Facebook. \n",
    "\n",
    "This umbrageous, but at the same time vast and interesting topic seemed perfect to be analyzed to have a better understanding of an unknown but still very big part the current consumption of internet.\n",
    "\n",
    "We are currently halfway the completion of the Data Analytics Bootcamp, a 9-weeks full-time course where Python, SQL, Pandas and Tableau are covered, and these tools together make a difference when trying to analyze the available data.\n",
    "\n",
    "In order to understand a bit better porn statistics, type of content and currents, we had a few questions we wanted to answer:\n",
    "1. Which are the main categories, or tags used at porn sites?\n",
    "2. Which are the most popular categories, and which ones have more content?\n",
    "3. Which are the best rated categories?\n",
    "4. Is there a correlation between the number of videos that a category has and the average views per video?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "For this project, we used three different sources to get the most diverse and complete information possible:\n",
    "1. **Redtube Api** [api.redtube.com] The official Redtube Api, which allowed a maximum of 30000 requests per day. 51901 rows of data.\n",
    "2. **Pornhub Database, csv file** from Kaggle [www.kaggle.com], with 191532 rows of data.\n",
    "3. **Xhamster Database, csv file** from Sexualitics [https://sexualitics.github.io/] with 785119 rows of data which we limited to 100000 for easy manipulation.\n",
    "\n",
    "The information decided to use and manage is the following:\n",
    "[Video title]: Name of the video.\n",
    "[Number of views]: Accumulated amount of views per video.\n",
    "[Rating]: Average rating per video, from 0.0 to 1000\n",
    "[Category 1]: Category or tag of the video.\n",
    "[Category 2]: Second category of the video.\n",
    "[Category 3]: Third category of the video.\n",
    "[Website]: Source of the video *Either Redtube, Pornhub, Xhamster. \n",
    "*A video has always a designated category [Category_1] but can also have others [Category_2] and [Category_3] which are optional. The original dataset varies from 1 to 20 categories in one video. We decided to limit it to three to standarize it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "Each database was accessed, wrangled and cleaned separately to create an uniform database aligned with the relevant needed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed libraries\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redtube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Redtube_list.txt', 'r') as filehandle:\n",
    "    df = json.load(filehandle)\n",
    "    \n",
    "video = []\n",
    "for i in df:\n",
    "    for key, value in i.items():\n",
    "        if key == 'video':\n",
    "            video.append(value)\n",
    "\n",
    "views = np.array([])\n",
    "rating = np.array([])\n",
    "titles = np.array([])\n",
    "\n",
    "for dict1 in video:\n",
    "    for key1, value1 in dict1.items():\n",
    "        if key1 == 'views':\n",
    "            views = np.append(views, np.array([value1]))\n",
    "        elif key1 == 'rating':\n",
    "            rating = np.append(rating, np.array([value1]))\n",
    "        elif key1 == 'title':\n",
    "            titles = np.append(titles, np.array([value1]))\n",
    "\n",
    "categories = pd.DataFrame(tags)\n",
    "\n",
    "arrays = {'Title': titles, 'Number_of_Views': views, 'Rating': rating}\n",
    "table = pd.DataFrame(data=arrays\n",
    "                     \n",
    "rt_table = pd.merge(table, categories, left_index=True, right_index=True)\n",
    "                     \n",
    "rt_table['Website'] = 'Redtube'\n",
    "                     \n",
    "rt_table = full_table.astype({'Title': str, 'Number_of_Views': int, 'Rating': float, 'Category_1': str, 'Category_2': str, 'Category_3': str, 'Website': str})\n",
    "                     \n",
    "rt_table.info()\n",
    "\n",
    "rt_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pornhub CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access to database and first sight.\n",
    "ph = pd.read_csv('/Users/Maria/Desktop/tumbzilla_labels.csv', index_col = 0)\n",
    "ph.columns\n",
    "\n",
    "#Taking out unneeded columns\n",
    "ph.drop(['img_source', 'video_link','tags', 'length', 'quality' ], axis = 1, inplace=True)\n",
    "\n",
    "def cast_to_list(categories):\n",
    "    if pd.isnull(categories):\n",
    "        return categories\n",
    "    return [item.strip() for item in categories.split(\"__\")]\n",
    "\n",
    "ph['categories']= ph['categories'].apply(cast_to_list)\n",
    "\n",
    "categories = ph['categories'].apply(pd.Series\n",
    "categories.drop(columns = [3,4,5,6,7,8,9,10,11], inplace = True)\n",
    "categories.rename(columns = {0:'Category_1', 1:'Category_2', 2:'Category_3'}, inplace = True)\n",
    "                                                                     \n",
    "ph_merged = ph.merge(right = categories, left_index = True, right_index=True)\n",
    "ph_merged['Website'] = \"Pornhub\"\n",
    "ph_merged.rename(columns = {'nb_views':'Number_of_Views','title':'Title','voting':'Rating'}, inplace = True)     \n",
    "ph_merged = ph_merged[['Title', 'Number_of_Views', 'Rating', 'Category_1', 'Category_2', 'Category_3', 'Website']]   \n",
    "                                    \n",
    "ph_merged.head()                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xHamster CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xhamster dataset comes from https://sexualitics.github.io/ which contains 786120 rows containing videos' data.\n",
    "#We select the columns we are interested in.\n",
    "df = pd.read_csv('../../xhamster.csv')\n",
    "df_clean = df[['id','title','channels','nb_views']]\n",
    "df_clean = df_clean[df_clean['channels'].notnull()]\n",
    "df_clean = df_clean['channels'].apply(pd.Series)\n",
    "\n",
    "#The category column contains all the categories together, divided by a coma.\n",
    "#In order to split the strings inside the column 'category' we do it separately:\n",
    "three_cat = df_new2[0].str.split(pat=',', n=4, expand=True)\n",
    "cat1 = pd.DataFrame(three_cat[0].str.strip(to_strip=\"[,'] \\n\")) \n",
    "cat2 = pd.DataFrame(three_cat[1].str.strip(to_strip=\"[,'] \\n\")) \n",
    "cat3 = pd.DataFrame(three_cat[2].str.strip(to_strip=\"[,'] \\n\"))\n",
    "\n",
    "#We create the ultimate cleaned database by taking the original df table\n",
    "xh_table = df[['title','nb_views','channels']]\n",
    "xh_table = xh_table[df_real['channels'].notnull()]\n",
    "\n",
    "#We append the cleaned Category columns to the ultimate chart\n",
    "xh_table['Category_1'] = cat1\n",
    "xh_table['Category_2'] = cat2\n",
    "xh_table['Category_3'] = cat3\n",
    "xh_table.drop('channels',axis=1,inplace=True)\n",
    "xh_table['Website'] = 'Xhamster'\n",
    "\n",
    "#As agreed with the team, we rename the columns by convention\n",
    "xh_table = xh_table.rename(columns={'title': 'Title', 'nb_views': 'Number_of_Views'})\n",
    "\n",
    "#As the original cleaned dataset is 700000 rows, we limit it to 100000 most viewed ones\n",
    "xh_100 = xh_table.sort_values('Number_of_Views',ascending=False)\n",
    "xh_100 = xh_100.head(100000)\n",
    "xh_100 = xh_100.drop('channels',axis=1)\n",
    "xh_100.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Master Dataset, Version 0, not as clean as expected\n",
    "With above actions, we made it to merge three different sources into one single dataset with a total of 343433 rows, with the information we needed to analyze.\n",
    "\n",
    "As our analysis starts with categories, it is important to consider each of the categories assigned to each video, which could be any number from one to three, so we can count the number of videos per category.\n",
    "\n",
    "At the same time, some categories had similar names which we could consider as duplicates -for example: 'Anal sex' and 'Anal' could count as the same one, or 'Lesbian' and 'Lesbians'-. An extra layer of cleaning was needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code of cleaning Categories come here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
